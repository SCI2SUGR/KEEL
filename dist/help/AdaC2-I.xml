<method>

	<name>Cost Sensitive Boosting with C4.5 Decision Tree as Base Classifier</name>

	<reference>  

		<ref>Y. Sun, M. Kamel, A. Wong, and Y. Wang. Cost-sensitive boosting for classification of imbalanced data. Pattern Recognition, 40(12):3358â€“3378, December 2007. </ref>

	</reference>

	<generalDescription>  

		<type>Classification model by boosted decision trees to use in imbalanced environments</type>

		<objective>To determine a decision tree that on the basis of answers to questions about the input attributes predicts correctly the value of the target attribute.
                The cost set-ups are taken into account in the boosting procedure.</objective>

		<howWork> AdaC2 is a boosting algorithm that produce an ensemble of decision trees (C4.5 in this case) from a set of given examples. Each instance
                has an associated cost depending on the class. It tries to solve the imbalance problem by increasing the weight of the instances from the minority
                class in each iteration of the boosting algorithm.

                It can be executed using the cost sensitive C4.5 or a resampling procedure after the weights update.
                </howWork>

		<parameterSpec>  

			<param>prune: is to prune or not prune the tree. It is a boolean value that determines if the algorithm applies a prune process after building the tree</param>
			<param>confidence: is the confidence level. It is a float value that determines what is the minimal confidence that must has a leaf in order to can be considered in the tree</param>
			<param>minItemsets is the minimum number of item-sets per leaf. It is an integer value that determines how much data instances must contain a leaf in order to can be created</param>
			<param>Number of Classifiers: is the maximum number of iterations that the boosting procedure will be executed</param>
                        <param>Train Method: is the method used to train the classifier in each iteration (using resampling or cost-sensitive classifier)</param>
                        <param>Cost Set-up: is the cost set-up, when ADAPTIVE the costs are selected considering the imbalance ratio, in other case they are manually set.</param>
                        <param>Cost Majority class: is the cost for missclassifying an instance from the majority class (if set-up is MANUAL).</param>
                        <param>Cost Minority class: is the cost for missclassifying an instance from the minority class (if set-up is MANUAL).</param>

		</parameterSpec>

		<properties>

			<continuous>Yes</continuous>

			<discretized>Yes</discretized>

			<integer>Yes</integer>

			<nominal>Yes</nominal>

			<valueLess>Yes</valueLess>

			<impreciseValue>No</impreciseValue>

		</properties>

	</generalDescription>

	<example>Problem type: Imbalanced
Method: Cost Sensitive Boosting with C4.5 Decision Tree as Base Classifier
Dataset: abalone9-18
Training set: abalone9-18-5-5-1tra.dat
Test set: abalone9-18-5-5-1tst.dat
Test Show results: TSTImb-AdaC2
Parameters: default values

After the execution of RunKeel.jar we can see the classification results for the test set:

G-mean in Training: 0.9686557752314763
F-mean in Training: 0.66
TPrate in Training: 1.0
G-mean in Test: 0.7036825151484182
F-mean in Test: 0.3448275862068966
TPrate in Test: 0.5555555555555556
Accuracy in training: 0.9417808219178082
Accuracy in test: 0.8707482993197279

We can also see the output and target classes for each case of the test set (result0.tst) in Experiment\Results\Imb-AdaC2.abalone9-18:

@relation  abalone9-18
@attribute Sex{M,F,I}
@attribute Length real[0.075,0.815]
@attribute Diameter real[0.055,0.65]
@attribute Height real[0.0,1.13]
@attribute Whole_weight real[0.0020,2.8255]
@attribute Shucked_weight real[0.0010,1.488]
@attribute Viscera_weight real[5.0E-4,0.76]
@attribute Shell_weight real[0.0015,1.005]
@attribute Rings{positive,negative}
@inputs Sex,Length,Diameter,Height,Whole_weight,Shucked_weight,Viscera_weight,Shell_weight
@outputs Rings
@data
negative negative
negative negative
negative negative
negative negative
negative negative
positive negative
negative positive
negative positive
negative negative
negative negative
positive positive
negative negative
negative negative
positive positive
positive positive
positive negative
negative negative
negative negative
negative negative
negative negative
negative positive
negative negative
negative negative
negative negative
negative negative
negative positive
negative negative
negative positive
negative negative
negative negative
negative negative
...
</example>

</method>
